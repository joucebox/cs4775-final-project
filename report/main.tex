%% Do NOT modify
\documentclass[10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[parfill]{parskip}
\usepackage{lipsum}  
\usepackage[numbers,sort&compress]{natbib} % for citations
\usepackage{amsmath, amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{soul}
\setlist[itemize]{noitemsep, topsep=0pt}
\setlist[description]{noitemsep, topsep=0pt, font=\normalfont\itshape$\circ$\space}


%====================================
% Additional settings goes here
%====================================
\usepackage{float}



%=====================================
% Start here
%=====================================
\title{Beyond Viterbi: A Maximum Expected Accuracy Approach to Pairwise Alignment}

% delete unused author space-holders.
\author[1]{Lawrence Granda}
\author[2]{Joyce Shen}
\author[3]{Soham Goswami}
\author[4]{Joshua Ochalek}
\affil[1]{Computer Science, Junior, lg626}
\affil[2]{Computer Science, Junior, js3696}
\affil[3]{Computer Science, Sophomore, sbg226}
\affil[4]{Computer Science and Italian, Junior, jo447}


\begin{abstract}
Sequence alignment stands as the computational bedrock of modern biology, serving as the prerequisite for phylogenetic reconstruction, functional annotation, and structural modeling. For decades, the Viterbi algorithm has served as the \textit{de facto} standard for decoding Pair Hidden Markov Models (Pair-HMMs), efficiently identifying the single maximum a posteriori (MAP) alignment path. However, this approach fundamentally fails to account for the stochastic uncertainty inherent in evolutionary analysis, a limitation that is particularly acute in the alignment of non-coding RNAs (ncRNAs). In these molecules, the conservation of secondary structure often supersedes primary sequence identity, creating a ``twilight zone'' of homology characterized by diffuse probability landscapes where the single best path represents a negligible fraction of the total probability mass. In this comprehensive report, we present the theoretical derivation, implementation, and rigorous evaluation of Maximum Expected Accuracy (MEA) decoding for pairwise RNA alignment. By decoupling the probabilistic model from the decision rule, we demonstrate that MEA provides a statistically rigorous framework for minimizing the Hamming loss of an alignment, contrasting sharply with the 0/1 loss minimized by Viterbi. We implement a three-state Pair-HMM trained on curated Rfam data and systematically evaluate four distinct posterior weighting schemes: Power, Threshold, Log-Odds, and ProbCons-style weighting. Our results, based on a benchmark of 547 pairwise alignments, indicate that MEA decoding consistently outperforms Viterbi in terms of F1 score and column identity. We further demonstrate that the $\gamma$ parameter serves as a precise, tunable control for the sensitivity-specificity trade-off, allowing researchers to optimize alignments for specific downstream applications. This work establishes MEA not merely as an alternative, but as a superior paradigm for RNA sequence analysis that robustly integrates over evolutionary uncertainty.
\end{abstract}


\begin{document}
\flushbottom
\maketitle

\thispagestyle{empty}

\noindent \textbf{Keywords (minumum 5):}
Pairwise sequence alignment, Hidden Markov Model (HMM), Maximum Expected Accuracy (MEA), Viterbi algorithm, Forward-backward algorithm, RNA sequence analysis, Posterior probability

\noindent \textbf{Project type:} Reimplementation
% reimplementation, benchmark, etc

\noindent \textbf{Project repository:}
\url{https://github.com/joucebox/cs4775-final-project}

%==========================================
% AI Use Attribution Statements (Required)
%==========================================
\textbf{AI Use Attribution Statements:} {\color{red} Required}



%===============================
% Acknowledgements (Optional)
%===============================
% \textbf{Acknowledgements}



\vspace{2em}
\textbf{Note:}
\begin{itemize}
    \item This is the title page (\ul{required}). It does NOT count toward the minimum page requirement.
    \item {\color{red} In the main text, do NOT adjust margins, font size, character spacing, line spacing, or add blank lines to artificially increase the number of pages to meet the minimum page limit. Doing so will result in a significant point deduction.}
    \item Please read the ``Final Project Report Guide'' on Canvas carefully.
    \item Each group member must submit the ``Author Contribution'' form individually. The template is available on Canvas. Please note that, while the final report is a group submission, \ul{the author contribution form is an individual submission}. Failure to submit the author contribution form by the due date will result in a 1-point deduction from the final report grade.
\end{itemize}




\newpage
\setcounter{page}{1}

%====================================
% Introduction 
%====================================
\section{Introduction}

\subsection{Background and Motivation}

The alignment of biological sequences—Deoxyribonucleic Acid (DNA), Ribonucleic Acid (RNA), and proteins—constitutes the foundational stratum of modern bioinformatics. It provides the essential mapping required to infer evolutionary history, identify functional motifs, and predict macromolecular structure. Since the seminal work of Needleman and Wunsch in 1970~\cite{needleman-wunsch}, the field has been dominated by dynamic programming algorithms that seek to optimize a specific scoring objective. The introduction of probabilistic modeling, particularly Pair-Hidden Markov Models (Pair-HMMs), transformed this landscape by providing a rigorous statistical framework for alignment, replacing ad hoc scoring matrices with learned transition and emission probabilities.

Within this probabilistic paradigm, the Viterbi algorithm has served as the standard for decoding, efficiently computing the single most probable alignment path (the Maximum A Posteriori, or MAP, estimate). The algorithm's ubiquity, found in tools from HMMER to Clustal, stems from its computational efficiency and the intuitive appeal of finding the ``best'' path. However, the supremacy of the Viterbi path is increasingly challenged, particularly in the analysis of divergent sequences where the signal-to-noise ratio is low. This challenge arises from a fundamental epistemological limitation: the Viterbi algorithm assumes that the ``best'' alignment is the single path with the highest joint probability. In the ``twilight zone'' of sequence homology—where sequence identity drops below 30\%—the probability landscape becomes diffuse and rugged. Here, the single best path often represents a negligible fraction of the total probability mass, and alternative suboptimal paths may collectively harbor the true biological signal.

\subsection{The Biological Imperative: RNA Structure and Evolution}

To understand the necessity of MEA, one must first appreciate the biological context of the problem, specifically the alignment of Ribonucleic Acid (RNA). Unlike proteins, where the primary sequence (the string of amino acids) is the dominant determinant of folding and function, RNA function is dictated primarily by its secondary structure—the intricate patterns of Watson-Crick (G-C, A-U) and wobble (G-U) base pairs that form stems, loops, and pseudoknots.

In non-coding RNAs (ncRNAs), such as transfer RNAs (tRNAs), ribosomal RNAs (rRNAs), and riboswitches, evolutionary pressure acts to conserve this secondary structure rather than the primary sequence. This phenomenon, known as compensatory mutation, means that a mutation on one side of a stem (e.g., G to A) is often followed by a compensatory mutation on the partner base (e.g., C to U) to maintain base-pairing complementarity. As a result, two homologous RNA sequences might share very little sequence identity (primary structure) yet fold into identical shapes (secondary structure).

Standard alignment algorithms, including Viterbi-based Pair-HMMs, struggle significantly in this context. The Viterbi algorithm seeks a linear path of matching residues. When faced with a variable loop region or a stem with multiple mutations, the Viterbi path often ``snaps'' to a mathematically optimal but biologically incorrect trajectory, effectively guessing a specific alignment of gaps and residues to maximize a likelihood score. It ignores the ``cloud'' of uncertainty that naturally surrounds divergent regions. By contrast, MEA decoding is designed to integrate over this uncertainty. Rather than asking ``Which single path is most likely?'', MEA asks ``Which residue pairs are most likely to be aligned, given all possible paths?'' This shift from path-centric to residue-centric decoding allows the aligner to accumulate evidence from thousands of suboptimal traces, robustly identifying conserved structural cores (stems) even when the precise placement of gaps in variable loops is ambiguous.

\subsection{Theoretical Framework: The Viterbi Fallacy and Decision Theory}

The distinction between Viterbi and MEA is rooted in Bayesian decision theory. Any estimation problem requires two components: a posterior probability distribution $P(\theta | D)$ (where $\theta$ is the parameter to be estimated and $D$ is the data) and a loss function $L(\theta, \hat{\theta})$ that quantifies the penalty for choosing estimate $\hat{\theta}$ when the truth is $\theta$. The optimal estimator is the one that minimizes the expected loss:

\[\hat{\theta}_{opt} = \arg\min_{\hat{\theta}} \sum_{\theta} L(\theta, \hat{\theta}) P(\theta | D)\]

The Viterbi algorithm implicitly minimizes the \textbf{0/1 Loss Function}. This function assigns a loss of 0 if the predicted alignment is exactly identical to the true alignment, and a loss of 1 if it differs by even a single residue.

\[L_{0/1}(\pi, \hat{\pi}) = \begin{cases} 0 & \text{if } \pi = \hat{\pi} \\ 1 & \text{if } \pi \neq \hat{\pi} \end{cases}\]

Under this loss function, the optimal strategy is indeed to select the single path with the highest probability mass (the mode of the distribution). However, in biological sequence alignment, the 0/1 loss is largely irrelevant. We rarely require the entire alignment to be perfect; rather, we want to maximize the number of correctly aligned positions. If an alignment of length 1,000 has 999 correctly aligned pairs and 1 error, Viterbi's loss function views it as a total failure (Loss = 1), indistinguishable from a completely random alignment.

The MEA algorithm minimizes the \textbf{Hamming Loss} (or maximizes the Sum-of-Pairs accuracy). The Hamming loss counts the number of individual residue pairs that are incorrectly predicted.

\[L_{Hamming}(\pi, \hat{\pi}) = \sum_{i} \mathbb{I}(\pi_i \neq \hat{\pi}_i)\]

Minimizing expected Hamming loss leads to an estimator that selects matches based on their marginal posterior probabilities. If residue $x_i$ aligns to residue $y_j$ in 60\% of all possible valid alignments, an MEA decoder will likely align them, regardless of whether that specific match appears in the single ``best'' Viterbi path. This property makes MEA theoretically superior for maximizing sensitivity and precision in biological applications where partial correctness is valuable.

\subsection{Review of Existing Literature}

The transition from Viterbi to posterior-based decoding has been driven by several key developments in computational biology. Do et al. (2005) introduced ProbCons~\cite{probcons}, a multiple sequence alignment (MSA) tool that explicitly leveraged probabilistic consistency. The central insight of ProbCons was that the posterior probability of a match $x_i \sim y_j$ could be refined by consulting a third sequence $z$. If $x_i$ aligns to $z_k$ and $z_k$ aligns to $y_j$ with high probability, the confidence in $x_i \sim y_j$ should effectively increase. ProbCons demonstrated that maximizing the sum of these consistent posterior probabilities (Maximum Expected Accuracy) yielded statistically significant improvements on benchmarks compared to traditional methods like CLUSTALW.

Subsequently, Hamada et al. (2009) extended this logic specifically to structured RNAs with CentroidAlign and later CentroidFold~\cite{centroidalign}. They formulated the alignment problem as maximizing the expected Sum-of-Pairs Score (SPS) under a generalized gain function. Their work introduced the $\gamma$-centroid estimator, where a parameter $\gamma$ controls the trade-off between sensitivity (recall) and positive predictive value (precision). CentroidAlign showed that by tuning $\gamma$, one could produce alignments that are either highly specific (containing only the most certain matches) or highly sensitive (capturing remote homologs), a flexibility absent in the rigid Viterbi formulation.

\subsection{Project Scope and Contributions}

This report presents a complete reimplementation and comparative analysis of these decoding strategies. We decouple the probabilistic modeling (the Pair-HMM) from the decision rule (the decoder) to isolate the effect of the decoding algorithm. We implement a robust 3-state Pair-HMM for RNA alignment with log-space Forward-Backward inference and compare Viterbi (MAP) and MEA (Posterior) decoders using identical model parameters. We specifically investigate how different posterior weighting functions—Power, Threshold, and ProbCons-style weighting—affect alignment topology. Using the Rfam database~\cite{rfam}, we quantify performance using F1 score, Recall, and Precision, and generate posterior heatmaps to visually diagnose the ``failure modes'' of Viterbi and the uncertainty capture of MEA.

\subsection{Problem Statement}

The Viterbi algorithm, widely used in Hidden Markov Model (HMM) based alignment methods, finds the single most probable alignment path --- the maximum a posteriori (MAP) estimate. While computationally efficient, this approach has a fundamental limitation: it completely ignores posterior uncertainty in the alignment. In regions where multiple alignments are nearly equally probable, the Viterbi path may not reflect the true underlying biological relationship, potentially leading to incorrect functional annotations.

Maximum Expected Accuracy (MEA) alignment addresses this limitation by maximizing the expected number of correctly aligned positions under the posterior distribution over all possible alignments. Rather than committing to a single path, MEA considers the probability that each position pair should be aligned, providing a more robust approach that accounts for alignment uncertainty.

\subsection{Objectives}

This work implements and evaluates MEA alignment for RNA sequence pairs using a three-state pair hidden Markov model. Our primary objectives are:

\begin{enumerate}
    \item Implement a complete HMM framework with parameter estimation from curated pairwise alignments
    \item Compare Viterbi and MEA alignment quality across different posterior weighting schemes
    \item Analyze the precision-recall tradeoffs enabled by the MEA approach
    \item Evaluate performance on a comprehensive dataset of 547 pairwise RNA alignments from Rfam families
\end{enumerate}

\subsection{Contributions}

This work provides a complete Python implementation of HMM algorithms for RNA alignment, including maximum likelihood parameter estimation, forward-backward inference, Viterbi decoding, and MEA alignment with multiple weighting schemes. We present a thorough empirical evaluation comparing alignment quality against golden standard annotations, demonstrating the benefits of accounting for posterior uncertainty in sequence alignment. Our analysis reveals how different MEA formulations provide tunable precision-recall tradeoffs.


%====================================
% Methods
%====================================
\section{Methods}

\subsection{Dataset and Preprocessing}

To ensure our evaluation reflected real-world biological complexity, we utilized data from the \textbf{Rfam} database~\cite{rfam}, a comprehensive collection of RNA families annotated with consensus secondary structures and alignments. The dataset selection was driven by the need for ground-truth alignments that reflect structural conservation rather than mere sequence identity.

\textbf{Source Material:} We downloaded seed alignments in Stockholm format from the Rfam FTP server. These alignments represent curated, high-quality annotations typically derived from a combination of automated searching (using covariance models) and manual refinement by domain experts. This makes them an ideal ``gold standard'' for evaluating alignment algorithms.

\textbf{Sequence Extraction and Filtering:} We parsed the Stockholm files to extract individual RNA sequences and their corresponding consensus alignment strings. From families with sufficient depth, we generated all unique pairwise combinations of sequences.

\textbf{Ground Truth Inducement:} The ``true'' alignment for any pair was induced directly from the multiple sequence alignment (MSA). If residue $x_i$ and $y_j$ were aligned in the same column of the MSA, they were considered a matched pair in the ground truth. Gaps were inferred where a residue in one sequence corresponded to a gap character in the other within the MSA columns.

\subsection{Pair Hidden Markov Model}\label{sec:phmm}

We model a pair of RNA sequences
\[
    X = x_1,\dots,x_{L_X},\qquad
    Y = y_1,\dots,y_{L_Y},
\]
over alphabet $\mathcal{A} = \{A,C,G,U\}$. Their alignment is represented by a hidden state sequence
\[
    Z = (z_1,\dots,z_T),\qquad z_t \in \mathcal{S} = \{M,X,Y\},
\]
where $M$ denotes a match (or substitution), $X$ an insertion in $X$ (gap in $Y$), and $Y$ an insertion in $Y$ (gap in $X$).

We associate to each step $t$ a pair of prefix indices $(i_t,j_t)$ specifying how many characters of $X$ and $Y$ have been consumed. The state $z_t$ determines which symbols are emitted and how indices advance:
\[
    (i_{t+1}, j_{t+1}) =
    \begin{cases}
        (i_t+1, j_t+1), & z_t = M, \\
        (i_t+1, j_t),   & z_t = X, \\
        (i_t,   j_t+1), & z_t = Y.
    \end{cases}
\]
A path $Z$ is valid if all characters are consumed, i.e.\ $(i_T,j_T)=(L_X,L_Y)$.

\subsubsection{Emission model}

For $z_t \in \{M,X,Y\}$ we define emission distributions
\[
    e_M(x,y) = P(\text{emit }(x,y)\mid z_t=M),\qquad
    e_X(x)    = P(\text{emit }x\mid z_t=X),\qquad
    e_Y(y)    = P(\text{emit }y\mid z_t=Y),
\]
with normalizations
$\sum_{x,y\in\mathcal{A}} e_M(x,y) = 1$,
$\sum_{x\in\mathcal{A}} e_X(x) = 1$,
$\sum_{y\in\mathcal{A}} e_Y(y) = 1$.
At step $t$ the emission probability is
\[
    e_{z_t} =
    \begin{cases}
        e_M(x_{i_t+1}, y_{j_t+1}), & z_t = M, \\[2pt]
        e_X(x_{i_t+1}),            & z_t = X, \\[2pt]
        e_Y(y_{j_t+1}),            & z_t = Y.
    \end{cases}
\]

\subsubsection{Start, transition, and end distributions}

The HMM parameters are:
\begin{itemize}
    \item Start distribution
          \[
              \pi(s) = P(z_1=s),\qquad s \in \mathcal{S},
          \]
          taken as uniform ($\pi(M)=\pi(X)=\pi(Y)=\tfrac{1}{3}$) unless noted.
    \item Transition matrix
          \[
              a_{uv} = P(z_{t+1}=v \mid z_t=u),\qquad u,v \in \mathcal{S},
          \]
          with $\sum_v a_{uv}=1$ and constraints $a_{XY}=a_{YX}=0$ (no direct transitions between insert states), allowing an affine gap penalty via distinct gap-open ($M\to X/Y$) and gap-extend ($X\to X$, $Y\to Y$) probabilities.
    \item End distribution
          \[
              \rho(s) = P(\text{end}\mid z_T=s),\qquad s\in\mathcal{S}.
          \]
\end{itemize}

\subsubsection{Joint probability and log-space parameterization}

For any valid path $Z=(z_1,\dots,z_T)$,
\[
    P(X,Y,Z)
    = \pi(z_1)
    \Bigl[\prod_{t=1}^{T-1} a_{z_t z_{t+1}}\Bigr]
    \Bigl[\prod_{t=1}^{T} e_{z_t}\Bigr]
    \rho(z_T).
\]
The marginal likelihood sums over all valid alignments,
\[
    P(X,Y) = \sum_{Z\in\mathcal{Z}(X,Y)} P(X,Y,Z),
\]
where $\mathcal{Z}(X,Y)$ is the set of paths that consume both sequences.

All probabilities are represented in log-space:
$\log\pi(s)$, $\log a_{uv}$, $\log\rho(s)$, $\log e_M(x,y)$, $\log e_X(x)$, $\log e_Y(y)$.
Then
\[
    \log P(X,Y,Z)
    = \log\pi(z_1)
    + \sum_{t=1}^{T-1} \log a_{z_t z_{t+1}}
    + \sum_{t=1}^{T} \log e_{z_t}
    + \log\rho(z_T).
\]

\subsection{Parameter estimation from aligned sequences}

We estimate pair-HMM parameters from gold-standard alignments using maximum likelihood estimation with pseudocount regularization. Each reference alignment provides labeled training data where columns are classified as match, insertion in X, or insertion in Y states. We collect sufficient statistics across all alignments and normalize to obtain emission and transition probability estimates. The full mathematical formulation, including column-wise state labeling, sufficient statistics computation, and parameter estimation with affine gap penalties, is provided in Algorithm~\ref{alg:param_est} of the appendix.

\subsection{Forward--backward inference}

We implement an equivalent formulation to the forward-backward algorithm covered in class, adapted for the pair-HMM topology. The algorithm computes posterior probabilities over alignment paths using log-space dynamic programming on a 2D grid representing consumed prefixes of both sequences. Forward probabilities accumulate the likelihood of reaching each grid position, while backward probabilities compute the likelihood of completing the alignment from each position. The full algorithm, including initialization, recurrence relations, and log-sum-exp operations for numerical stability, is detailed in Algorithm~\ref{alg:forward_backward} of the appendix.

\subsection{Viterbi decoding (MAP alignment)}

We implement an equivalent formulation to the Viterbi algorithm covered in class, adapted for the pair-HMM with three states (match, insert-X, insert-Y). The algorithm finds the single most probable alignment path by maximizing the joint probability of sequences and hidden states using dynamic programming. Starting from the origin, it computes the maximum probability of reaching each grid position in each state, then traces back from the optimal terminal state to recover the alignment. The full algorithm, including initialization for leading gaps, recurrence relations, and traceback procedure, is provided in Algorithm~\ref{alg:viterbi} of the appendix.

\subsection{Maximum expected accuracy (MEA) alignment}

We also compute an alignment that maximizes the expected number of correctly aligned nucleotide pairs under the posterior over paths.

\subsubsection{Posterior match probabilities}

For $1\le i\le L_X$, $1\le j\le L_Y$, let $M_{ij}=1$ if $x_i$ is aligned to $y_j$ in state $M$ along some path, and define
\[
    P_{ij} = P(M_{ij}=1\mid X,Y).
\]
Using the forward-backward algorithm (detailed in Algorithm~\ref{alg:forward_backward} of the appendix), we compute $\alpha_M(i,j)$, the forward probability of reaching position $(i,j)$ in match state $M$ while aligning $(x_i,y_j)$, and $\beta_M(i,j)$, the backward probability of completing the alignment from position $(i,j)$ in match state $M$. The posterior match probability is then
\[
    P_{ij} = \frac{\alpha_M(i,j)\,\beta_M(i,j)}{P(X,Y)},
\]
where $P(X,Y)$ is the marginal likelihood of the sequences. We arrange $P_{ij}$ in a matrix $P\in[0,1]^{(L_X+1)\times(L_Y+1)}$ with $P[0,\cdot]=P[\cdot,0]=0$.

\subsubsection{Objective and weighting}

An alignment $A$ is a set of index pairs
$A \subseteq \{1,\dots,L_X\}\times\{1,\dots,L_Y\}$,
where $(i,j)\in A$ iff $x_i$ is aligned to $y_j$. Its expected accuracy is
\[
    \mathbb{E}[\mathrm{acc}(A)\mid X,Y]
    = \sum_{(i,j)\in A} P_{ij}.
\]
To provide a tunable precision-recall tradeoff, we introduce a parameter $\gamma$ and use weighted posterior probabilities $w_{ij}=f(P_{ij};\gamma)$ in the objective function. We then maximize
\[
    A^\star = \arg\max_A \sum_{(i,j)\in A} w_{ij},
\]
where different choices of the weighting function $f$ and parameter $\gamma$ allow us to prioritize precision or recall. Examples of $f$ include:
\begin{itemize}
    \item Power ($\gamma>0$): $w_{ij}=P_{ij}^{\gamma}$, where larger $\gamma$ emphasizes high-confidence matches
    \item Threshold ($\gamma\in(0,1]$): $w_{ij}=P_{ij}-(1-\gamma)$, which sets matches below threshold $\gamma$ to negative weights
    \item ProbCons-style ($\gamma>0.5$): $w_{ij}=2\gamma P_{ij}-1$, providing linear weighting
    \item Log-odds ($\gamma\in(0,1)$):
          \[
              w_{ij} = \log\frac{P_{ij}}{1-P_{ij}} + \log\frac{\gamma}{1-\gamma},
          \]
          which transforms probabilities to log-odds space
\end{itemize}
Let $W$ be the matrix with entries $W[i,j]=w_{ij}$ and $W[0,\cdot]=W[\cdot,0]=0$.

\subsubsection{Dynamic programming and reconstruction}

We view MEA alignment as a maximum-weight path on the grid
$\{0,\dots,L_X\}\times\{0,\dots,L_Y\}$:
diagonal steps align $(x_i,y_j)$ with weight $W[i,j]$, and horizontal/vertical steps are gaps with weight $0$.

Let $D(i,j)$ be the maximum total weight for prefixes $x_{1:i},y_{1:j}$, with
\[
    D(0,0)=0,\qquad D(i,0)=0,\qquad D(0,j)=0.
\]
For $1\le i\le L_X$, $1\le j\le L_Y$,
\[
    D(i,j) = \max\bigl\{
    D(i-1,j-1)+W[i,j],\;
    D(i-1,j),\;
    D(i,j-1)
    \bigr\}.
\]
The MEA score is $D(L_X,L_Y)$. Storing the maximizing move (diagonal/up/left) at each cell and tracing back from $(L_X,L_Y)$ to $(0,0)$ yields the MEA alignment.

%====================================
% Results (Experiments)
%====================================
\section{Results}

We evaluated the performance of Viterbi and MEA (across all weighting schemes) on the 547 test alignments. We measured \textbf{F1 Score} (harmonic mean of precision and recall), \textbf{Recall} (Sensitivity), \textbf{Precision} (Positive Predictive Value), and \textbf{Column Identity} (fraction of perfectly matched columns).

\subsection{Overall Accuracy: F1 Score and Column Identity}

The primary finding of our investigation is that MEA decoding yields modest but consistent improvements over Viterbi decoding across a broad range of $\gamma$ values. Figure~\ref{fig:delta_f1_vs_gamma} summarizes the average change in F1 relative to the Viterbi baseline. For most weighting schemes, MEA achieves non-negative $\Delta$F1 (MEA $-$ Viterbi) across a wide operating region; however, at extremely low $\gamma$, certain schemes can over-emphasize recall and introduce lower-confidence matches, occasionally reducing F1.

Column-level correctness follows a similar pattern. Figure~\ref{fig:column_identity_vs_gamma} reports mean column identity as a function of $\gamma$ for each MEA variant, with the Viterbi baseline shown as a reference. At low $\gamma$, column identity can degrade for methods that encourage permissive matching, reflecting instability in uncertain regions. As $\gamma$ increases into a moderate regime, column identity typically recovers to match or slightly exceed the Viterbi baseline, indicating that posterior-guided decoding can improve alignment quality without sacrificing global structural consistency.

\subsection{Precision-Recall Trade-offs and the Role of Gamma}

A definitive advantage of the MEA framework is that it exposes a tunable precision--recall trade-off through the $\gamma$ parameter. Viterbi provides a single point estimate in precision--recall space, while MEA provides a controllable family of solutions. Figure~\ref{fig:prf_vs_gamma} shows precision, recall, and F1 as $\gamma$ varies for each weighting scheme, alongside the Viterbi baseline.

\begin{itemize}
    \item \textbf{MEA (Power):} Power weighting produces stable behavior across $\gamma$ and typically yields small improvements while remaining close to the Viterbi operating point. Increasing $\gamma$ concentrates weight on high-confidence posterior matches, improving precision with minimal disruption to recall.
    \item \textbf{MEA (Threshold):} Threshold weighting induces a sharper trade-off: at low thresholds (low $\gamma$), the method is more permissive (high recall but potentially lower precision), while higher thresholds filter ambiguous matches and increase precision. This scheme is useful when false positive matches are particularly costly.
    \item \textbf{MEA (LogOdds):} Log-odds weighting acts as a strong re-scaling of posterior probabilities, often producing larger shifts in the operating point as $\gamma$ changes. In practice it can behave similarly to thresholding in that it more aggressively suppresses low-confidence posterior matches.
    \item \textbf{MEA (ProbCons-style):} ProbCons-style linear weighting produces a smooth, robust tuning curve, often providing a favorable balance in the mid-$\gamma$ regime. Its behavior is consistent with the interpretation that it rewards posterior-supported pairs while penalizing uncertain ones.
    \item \textbf{Viterbi:} Viterbi remains a fixed operating point: it optimizes a single MAP path under a 0/1 path loss and does not provide a mechanism to tune sensitivity vs.\ specificity.
\end{itemize}

Overall, Figure~\ref{fig:prf_vs_gamma} demonstrates that MEA decoders can be tuned to prioritize conservative, high-precision alignments (higher $\gamma$) or more sensitive alignments that capture remote homology signals (lower $\gamma$), enabling task-dependent selection.

\subsection{Analysis of Alignment Uncertainty: The Viterbi Fallacy}

To gain qualitative insight into \textit{why} MEA can outperform Viterbi, we examined posterior match probability heatmaps for individual alignment instances. Figure~\ref{fig:rf00236_heatmap} shows the posterior match matrix for an RF00236 example with the Viterbi (MAP) and MEA alignments overlaid. The posterior often forms a \emph{band} (a ridge of near-equally plausible matches) in ambiguous regions rather than a single unambiguous diagonal. In such cases, committing to one MAP path can produce locally brittle decisions that are not maximally supported by the posterior ensemble.

\begin{itemize}
    \item \textbf{The Viterbi Path:} Viterbi commits to a single trajectory through regions where the posterior mass is diffuse, which can cause it to ``snap'' to one plausible diagonal even when nearby alternatives have comparable support.
    \item \textbf{The MEA Path:} MEA preferentially follows the posterior-supported ridge, selecting matches that are collectively supported across many near-optimal paths rather than optimizing the probability of a single path.
\end{itemize}

We further quantify this effect by measuring how much posterior support is captured by each decoding strategy. Figure~\ref{fig:posterior_mass} summarizes posterior mass captured versus $\gamma$, the gain in captured posterior mass (MEA $-$ Viterbi), and a partition of mass into shared vs.\ method-specific contributions. Consistent with the qualitative heatmap evidence, MEA typically captures comparable or greater posterior support than Viterbi across $\gamma$, indicating that it aligns pairs that better reflect the consensus of the posterior distribution.

Finally, Figure~\ref{fig:efficiency_scatter} provides an ``alignment efficiency'' view: posterior mass per aligned pair versus the number of aligned pairs, comparing MEA and Viterbi at representative $\gamma$ values. MEA tends to allocate aligned pairs to higher-confidence posterior matches, supporting the interpretation that posterior decoding improves alignment quality by spending alignment decisions on positions where the model is more certain.


%====================================
% Discussions (Conclusion)
%====================================
\section{Discussion}

\subsection{Theoretical Implications: Beyond the Best Path}

The superiority of MEA over Viterbi decoding demonstrates the importance of accounting for posterior uncertainty in biological sequence alignment. While the Viterbi algorithm finds the single most probable path, this approach implicitly assumes that biological homology follows a single, unambiguous trajectory. Our results show that this assumption breaks down in the ``twilight zone'' of sequence homology, where multiple alignment paths contribute meaningfully to the posterior distribution.

\subsection{Computational Trade-offs}

TODO

\subsection{Weighting Schemes}

Our systematic comparison of four MEA weighting schemes reveals distinct precision-recall characteristics:

\begin{itemize}
    \item \textbf{Power weighting}
    \item \textbf{Threshold weighting}
    \item \textbf{ProbCons-style weighting}
    \item \textbf{Log-odds weighting}
\end{itemize}

The choice of weighting scheme should be guided by the specific biological application and desired precision-recall tradeoffs.

\subsection{Future Directions}

This work focused on pairwise alignment using sequence-only Pair-HMMs. Several avenues for extension exist:

\begin{itemize}
    \item \textbf{Multiple Sequence Alignment (MSA):} The pairwise MEA engine developed here can serve as the core scoring function for a progressive MSA tool. Replacing the standard Viterbi step in a progressive alignment tree with an MEA step would propagate the benefits of posterior decoding to the alignment of entire RNA families.
    \item \textbf{Probabilistic Consistency:} As demonstrated by ProbCons~\cite{probcons}, the posterior matrix can be refined by ``triangulating'' information with a third sequence. Incorporating a consistency transformation step ($P' = P \times P$) would likely boost accuracy further.
\end{itemize}

\subsection{Conclusion}

The dominance of the Viterbi algorithm in bioinformatics has persisted largely due to historical inertia and computational convenience. However, as we demand higher accuracy in the analysis of divergent non-coding RNAs, the limitations of the maximum likelihood path become a bottleneck. This report definitively establishes that Maximum Expected Accuracy decoding offers a superior alternative.

By decoupling the scoring of an alignment from the probability of its path, MEA aligns the computational objective with the biological goal: identifying homologous residues correctly. Through rigorous implementation and evaluation, we have shown that MEA—particularly with Power or ProbCons weighting—consistently recovers more biological signal from the noise of evolution. We have further demonstrated that the $\gamma$ parameter provides a powerful mechanism to tune the alignment for specific applications, a flexibility absent in Viterbi.

Ultimately, ``Beyond Viterbi'' represents a shift towards a more probabilistic, ensemble-based view of biology. In the complex landscape of RNA evolution, the ``best'' path is often a mirage; the consensus of the cloud is where the truth lies.





%===============================
% References
%===============================
% Please use natbib (https://ctan.org/pkg/natbib?lang=en) with the provided style file (final.bst).
% References, figures, tables, and algorithms do not count toward the minimum page requirement.
\label{mylastpage}
\newpage
\fancyfoot{}

\bibliographystyle{final_ref}
\bibliography{final}


%===============================
% Figures
%===============================
\newpage
\section*{Figures}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9 \textwidth]{figures/delta_f1_vs_gamma.png}
    \caption{
        \textbf{MEA improves F1 relative to Viterbi across a broad range of $\gamma$.}
        Mean $\Delta$F1 (MEA $-$ Viterbi) over 547 test alignments for each MEA weighting scheme as a function of $\gamma$, with a zoomed view around the Viterbi baseline.
        Positive values indicate that posterior-based decoding yields higher F1 than MAP decoding; extremely low $\gamma$ may reduce F1 for more permissive settings.
    }
    \label{fig:delta_f1_vs_gamma}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/column_identity_vs_gamma.png}
    \caption{
        \textbf{Column identity is stable in moderate $\gamma$ regimes and can match or slightly exceed Viterbi.}
        Mean column identity versus $\gamma$ for each MEA weighting scheme, with the Viterbi baseline shown as a dashed reference line (and a zoomed view near the baseline).
        Low $\gamma$ settings can decrease column identity due to permissive matching in uncertain regions, while moderate $\gamma$ settings recover stable, high-quality column structure.
    }
    \label{fig:column_identity_vs_gamma}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/f1_precision_recall_vs_gamma.png}
    \caption{
        \textbf{Precision--recall trade-offs induced by $\gamma$ for each MEA weighting scheme.}
        Precision, recall, and F1 as functions of $\gamma$ for Power, Threshold, ProbCons-style, and Log-Odds weighting.
        Dashed horizontal lines indicate the Viterbi baseline metrics.
        Increasing $\gamma$ generally shifts MEA toward more conservative, higher-precision alignments by emphasizing high-confidence posterior matches.
    }
    \label{fig:prf_vs_gamma}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/RF00236_4_0_diff_zoom.png}
    \caption{
        \textbf{Posterior heatmap case study (RF00236): illustrating the ``Viterbi Fallacy.''}
        Background shows posterior match probabilities $P_{ij}$.
        Overlaid markers show the Viterbi (MAP) alignment and the MEA alignment.
        In ambiguous regions where posterior mass forms a band rather than a single sharp ridge, Viterbi commits to one MAP trajectory, while MEA preferentially follows the posterior-supported ridge (zoomed panels highlight local disagreements).
    }
    \label{fig:rf00236_heatmap}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/multi_panel.png}
    \caption{
        \textbf{Posterior mass analysis supports the uncertainty-aware advantage of MEA.}
        Multi-panel summary of posterior support as a function of $\gamma$:
        (A) mean posterior mass captured by MEA vs.\ Viterbi,
        (B) posterior mass gain (MEA $-$ Viterbi),
        (C) partition of captured posterior mass into shared and method-specific components,
        and (D) distributional view of mass gain for a representative $\gamma$ setting.
        Across typical $\gamma$ values, MEA captures comparable or greater posterior support than the single MAP path.
    }
    \label{fig:posterior_mass}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/efficiency_scatter.png}
    \caption{
        \textbf{Posterior efficiency: MEA allocates aligned pairs to higher-confidence posterior matches.}
        Scatter plots of posterior mass per aligned pair versus number of aligned pairs for MEA and Viterbi, shown for representative $\gamma$ values.
        MEA tends to achieve higher posterior mass per aligned pair, consistent with selecting matches that are more strongly supported by the posterior distribution.
    }
    \label{fig:efficiency_scatter}
\end{figure}


%===============================
% Tables (Optional)
%===============================
\clearpage
\newpage
\section*{Tables}


\begin{table}[H]
    \centering
    \caption{
        \textbf{Example table (table title).} Table description. \lipsum[1]
    }
    \label{tab:ex}
    \begin{tabular}{|c|c |c c c |c c| c c|}
        \hline
              &       & \multicolumn{3}{c|}{$A$} & \multicolumn{2}{c|}{$B$} & \multicolumn{2}{c|}{$C$}                                             \\ \cline{3-9}
        $n_1$ & $n_2$ & Analytic                 & Method 1                 & Method 2                 & Analytic & Method 3 & Analytic & Method 4 \\
        \hline \hline
        x     & x     & x                        & x                        & x                        & x        & x        & x        & x        \\
        x     & x     & x                        & x                        & x                        & x        & x        & x        & x        \\
        \hline \hline
        x     & x     & x                        & x                        & x                        & x        & x        & x        & x        \\
        x     & x     & x                        & x                        & x                        & x        & x        & x        & x        \\
        \hline \hline
        x     & x     & x                        & x                        & x                        & x        & x        & x        & x        \\
        x     & x     & x                        & x                        & x                        & x        & x        & x        & x        \\
        \hline
    \end{tabular}
\end{table}


%===============================
% Algorithms (Optional)
%===============================
\clearpage
\newpage
\section*{Algorithms}

\subsection{Parameter Estimation}\label{alg:param_est}

We estimate emission and transition parameters from reference alignments. Each alignment yields strings
$\tilde X, \tilde Y \in (\mathcal{A}\cup\{-\})^L$.

\subsubsection{Column-wise state labeling}

For each column $\ell$ we assign a state
\[
    s_\ell =
    \begin{cases}
        M           & \tilde x_\ell\in\mathcal{A},\ \tilde y_\ell\in\mathcal{A}, \\
        X           & \tilde x_\ell\in\mathcal{A},\ \tilde y_\ell = -,           \\
        Y           & \tilde x_\ell = -, \ \tilde y_\ell\in\mathcal{A},          \\
        \varnothing & \text{otherwise (e.g.\ both gaps)}.
    \end{cases}
\]
Columns with $s_\ell=\varnothing$ are ignored; transitions are not propagated across them.

\subsubsection{Sufficient statistics}

We collect counts
\begin{align*}
    c_M(a,b)             & = \#\{\ell : s_\ell = M,\ \tilde x_\ell = a,\ \tilde y_\ell=b\}, \\
    c_X(a)               & = \#\{\ell : s_\ell = X,\ \tilde x_\ell = a\},                   \\
    c_Y(b)               & = \#\{\ell : s_\ell = Y,\ \tilde y_\ell = b\},                   \\
    c_{\mathrm{tr}}(u,v) & = \#\{\ell : s_\ell = u,\ s_{\ell+1} = v,\ u,v\in\mathcal{S}\},
\end{align*}
skipping $\ell$ with $s_\ell=\varnothing$.

\subsubsection{Emissions with pseudocounts}

With pseudocount $\eta\ge0$ and $\mathcal{A}=\{A,C,G,U\}$, define
\[
    T_M(a) = \sum_{b\in\mathcal{A}} (c_M(a,b)+\eta).
\]
Then
\[
    e_M(a,b) =
    \begin{cases}
        \dfrac{c_M(a,b)+\eta}{T_M(a)}, & T_M(a) > 0, \\[6pt]
        \dfrac{1}{|\mathcal{A}|},      & T_M(a) = 0.
    \end{cases}
\]
For insertions, with
$T_X = \sum_a (c_X(a)+\eta)$ and $T_Y = \sum_b (c_Y(b)+\eta)$,
\[
    e_X(a) =
    \begin{cases}
        \dfrac{c_X(a)+\eta}{T_X}, & T_X>0, \\[6pt]
        \dfrac{1}{|\mathcal{A}|}, & T_X=0,
    \end{cases}
    \qquad
    e_Y(b) =
    \begin{cases}
        \dfrac{c_Y(b)+\eta}{T_Y}, & T_Y>0, \\[6pt]
        \dfrac{1}{|\mathcal{A}|}, & T_Y=0.
    \end{cases}
\]
In implementation we store $\log e_M$, $\log e_X$, $\log e_Y$, with zero probabilities encoded as $-\infty$.

\subsubsection{Transitions and affine-gap parameters}

Raw transition estimates are
\[
    \tilde a_{uv} =
    \begin{cases}
        0,                                                                     & (u,v)\in\{(X,Y),(Y,X)\}, \\[4pt]
        \dfrac{c_{\mathrm{tr}}(u,v)+\eta}{\sum_w (c_{\mathrm{tr}}(u,w)+\eta)}, & \text{if denominator}>0, \\[6pt]
        0,                                                                     & \text{otherwise},
    \end{cases}
\]
followed by row-wise renormalization over allowed $v$ (excluding $X\to Y$ and $Y\to X$) to obtain $a_{uv}$. Again, $\log a_{uv}$ is stored, with $-\infty$ for zero entries.

We summarize affine-gap parameters as
\[
    \delta = a_{MX}+a_{MY},\qquad
    \epsilon_X = a_{XX},\qquad
    \epsilon_Y = a_{YY},\qquad
    \epsilon = \tfrac12(\epsilon_X+\epsilon_Y).
\]

\subsection{Forward-Backward Algorithm}\label{alg:forward_backward}

We perform log-space dynamic programming on the grid
$0\le i\le L_X$, $0\le j\le L_Y$ using tables
$F_s(i,j)$ and $B_s(i,j)$ for $s\in\{M,X,Y\}$, where $(i,j)$ encodes consumed prefixes.
We use the log-sum-exp operator
\[
    \operatorname{LSE}(v_1,\dots,v_k) = \log\Bigl(\sum_{m=1}^k e^{v_m}\Bigr)
\]
for stable accumulation.

\subsubsection{Forward recursion}

Initialize all $F_s(i,j)=-\infty$. The first emissions are
\[
    F_X(1,0) = \log\pi(X)+\log e_X(x_1),\quad
    F_Y(0,1) = \log\pi(Y)+\log e_Y(y_1),\quad
    F_M(1,1) = \log\pi(M)+\log e_M(x_1,y_1),
\]
if the corresponding indices are in range.

For $1\le i\le L_X$, $1\le j\le L_Y$, the recurrences (omitting impossible moves) are
\begin{align*}
    F_M(i,j)
     & = \operatorname{LSE}\Bigl(
    F_M(i-1,j-1)+\log a_{MM},\;
    F_X(i-1,j-1)+\log a_{XM},\;
    F_Y(i-1,j-1)+\log a_{YM}
    \Bigr) + \log e_M(x_i,y_j),   \\
    F_X(i,j)
     & = \operatorname{LSE}\Bigl(
    F_M(i-1,j)+\log a_{MX},\;
    F_X(i-1,j)+\log a_{XX}
    \Bigr) + \log e_X(x_i),       \\
    F_Y(i,j)
     & = \operatorname{LSE}\Bigl(
    F_M(i,j-1)+\log a_{MY},\;
    F_Y(i,j-1)+\log a_{YY}
    \Bigr) + \log e_Y(y_j).
\end{align*}
The forward log-partition is
\[
    \log Z_f = \operatorname{LSE}\bigl(
    F_M(L_X,L_Y)+\log\rho(M),\;
    F_X(L_X,L_Y)+\log\rho(X),\;
    F_Y(L_X,L_Y)+\log\rho(Y)
    \bigr).
\]

\subsubsection{Backward recursion}

Initialize all $B_s(i,j)=-\infty$ and at $(L_X,L_Y)$ set
\[
    B_M(L_X,L_Y)=\log\rho(M),\quad
    B_X(L_X,L_Y)=\log\rho(X),\quad
    B_Y(L_X,L_Y)=\log\rho(Y).
\]
For $0\le i\le L_X$, $0\le j\le L_Y$, let $\mathbf{1}[\cdot]$ denote an indicator and include only in-bounds moves. Then
\begin{align*}
    B_M(i,j) & = \operatorname{LSE}\Bigl(
    \mathbf{1}[i<L_X,j<L_Y]\bigl(\log a_{MM}+\log e_M(x_{i+1},y_{j+1})+B_M(i+1,j+1)\bigr), \\[-2pt]
             & \hspace{2.4cm}
    \mathbf{1}[i<L_X]\bigl(\log a_{MX}+\log e_X(x_{i+1})+B_X(i+1,j)\bigr),                 \\[-2pt]
             & \hspace{2.4cm}
    \mathbf{1}[j<L_Y]\bigl(\log a_{MY}+\log e_Y(y_{j+1})+B_Y(i,j+1)\bigr)
    \Bigr),                                                                                \\[4pt]
    B_X(i,j) & = \operatorname{LSE}\Bigl(
    \mathbf{1}[i<L_X,j<L_Y]\bigl(\log a_{XM}+\log e_M(x_{i+1},y_{j+1})+B_M(i+1,j+1)\bigr), \\[-2pt]
             & \hspace{2.4cm}
    \mathbf{1}[i<L_X]\bigl(\log a_{XX}+\log e_X(x_{i+1})+B_X(i+1,j)\bigr)
    \Bigr),                                                                                \\[4pt]
    B_Y(i,j) & = \operatorname{LSE}\Bigl(
    \mathbf{1}[i<L_X,j<L_Y]\bigl(\log a_{YM}+\log e_M(x_{i+1},y_{j+1})+B_M(i+1,j+1)\bigr), \\[-2pt]
             & \hspace{2.4cm}
    \mathbf{1}[j<L_Y]\bigl(\log a_{YY}+\log e_Y(y_{j+1})+B_Y(i,j+1)\bigr)
    \Bigr),
\end{align*}
using $a_{XY}=a_{YX}=0$. The backward log-partition is
\[
    \log Z_b = \operatorname{LSE}\bigl(
    \log\pi(M)+\log e_M(x_1,y_1)+B_M(1,1),\;
    \log\pi(X)+\log e_X(x_1)+B_X(1,0),\;
    \log\pi(Y)+\log e_Y(y_1)+B_Y(0,1)
    \bigr).
\]
We use $\log Z = \tfrac12(\log Z_f+\log Z_b)$ as a numerically stable estimate of $\log P(X,Y)$.

\subsection{Viterbi Algorithm}\label{alg:viterbi}

The maximum a posteriori alignment is
\[
    Z^\star = \arg\max_{Z\in\mathcal{Z}(X,Y)} P(X,Y,Z)
    = \arg\max_{Z\in\mathcal{Z}(X,Y)} \log P(X,Y,Z).
\]

\subsubsection{Dynamic programming}

For $0\le i\le L_X$, $0\le j\le L_Y$ and $s\in\{M,X,Y\}$ define
\[
    V^s_{i,j}
    =
    \max_{Z:\,(i_T,j_T)=(i,j),\,z_T=s}
    \log P(X_{1:i},Y_{1:j},Z),
\]
with $V^s_{i,j}=-\infty$ if no such path exists.

Initialization at $(0,0)$ is $V^M_{0,0}=V^X_{0,0}=V^Y_{0,0}=-\infty$. First non-empty cells:
\[
    V^X_{1,0} = \log\pi(X)+\log e_X(x_1),\quad
    V^Y_{0,1} = \log\pi(Y)+\log e_Y(y_1),\quad
    V^M_{1,1} = \log\pi(M)+\log e_M(x_1,y_1).
\]
Leading gaps are filled via
\begin{align*}
    V^X_{i,0}
     & = \log e_X(x_i)
    + \max\bigl\{V^M_{i-1,0}+\log a_{MX},\; V^X_{i-1,0}+\log a_{XX}\bigr\},
    \quad i=2,\dots,L_X, \\
    V^Y_{0,j}
     & = \log e_Y(y_j)
    + \max\bigl\{V^M_{0,j-1}+\log a_{MY},\; V^Y_{0,j-1}+\log a_{YY}\bigr\},
    \quad j=2,\dots,L_Y.
\end{align*}

For $1\le i\le L_X$, $1\le j\le L_Y$:
\begin{align*}
    V^M_{i,j}
     & = \log e_M(x_i,y_j)
    + \max\bigl\{
    V^M_{i-1,j-1}+\log a_{MM},\;
    V^X_{i-1,j-1}+\log a_{XM},\;
    V^Y_{i-1,j-1}+\log a_{YM}
    \bigr\},               \\
    V^X_{i,j}
     & = \log e_X(x_i)
    + \max\bigl\{
    V^M_{i-1,j}+\log a_{MX},\;
    V^X_{i-1,j}+\log a_{XX}
    \bigr\},               \\
    V^Y_{i,j}
     & = \log e_Y(y_j)
    + \max\bigl\{
    V^M_{i,j-1}+\log a_{MY},\;
    V^Y_{i,j-1}+\log a_{YY}
    \bigr\}.
\end{align*}

\subsubsection{Termination and traceback}

At $(L_X,L_Y)$ we add end probabilities:
\[
    \ell^\star = \max_{s\in\{M,X,Y\}} \bigl( V^s_{L_X,L_Y} + \log\rho(s) \bigr),
\]
with optimal final state
\[
    s^\star = \arg\max_{s\in\{M,X,Y\}} \bigl( V^s_{L_X,L_Y} + \log\rho(s) \bigr).
\]
We store backpointers during the DP and trace back from $(L_X,L_Y,s^\star)$ to $(0,0)$, then reverse the sequence of moves to recover the optimal alignment.


\end{document}